{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "# Introduction to PySpark: Spark SQL<a name=\"id1\"></a>\n",
    "\n",
    "[Spark SQL](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html) is an Apache Spark module for structured data processing (Dataframe and Dataset). The main advantage of Spark SQL is that uses the data structure information for improving the data processing.\n",
    "\n",
    "There are two ways of interacting with Spark SQL:\n",
    "* SQL queries: read data from a Database like Hive (for Big Data). Query result is provided as a Dataset/Dataframe. \n",
    "* Dataset API (Application Programming Interface):\n",
    "    * Dataset is a distributed data collection. This new data paradigm was created in Spark 1.6. Dataset API id available for Scala and Java, but not for python.\n",
    "    * Dataframe is a column-organized Dataset. It is similar to a relational database table, or an optimized R/Python Dataframe. Dataframes can be created from structured data files, or external databases. Datframe API is available in Scala, Java, Python and R. E\n",
    "    \n",
    "SQL PySpark module contains the following classes:\n",
    "    \n",
    "* `pyspark.sql.SparkSession`: Main starting point to use Spark and DataFrame API.\n",
    "* `pyspark.sql.DataFrame`: Distributed data collection grouped by columns.\n",
    "* `pyspark.sql.Column`: DataFrame column\n",
    "* `pyspark.sql.functions`: List of functions available for DataFrames (min, max, col, mean...)\n",
    "* `pyspark.sql.GroupedData`: Agregation methods\n",
    "\n",
    "\n",
    "* `pyspark.sql.Row`: Row in a Dataframe\n",
    "* `pyspark.sql.DataFrameNaFunctions`: Methods to handle null or nan data\n",
    "* `pyspark.sql.DataFrameStatFunctions`: Statistics methods.\n",
    "* `pyspark.sql.types`: Available data types list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting PySpark<a name=\"id2\"></a>\n",
    "\n",
    "Before begining to work with Spark SQL we must initiate Spark session. Since Spark version 2.0 **SparkSession** is the starting point for PySpark.\n",
    "\n",
    "SparkSession is the starting point to create any PySpark functionality like DataFrames. \n",
    "\n",
    "To create an Spark session, se must use this command - `SparkSession.builder()`:\n",
    "\n",
    "* `appName(nombre_de_la_app)`: application name to identify in the Spark User Interface. If the input is blank, a random name is selected.\n",
    "* `config(opciones de spark)`: configuration options.\n",
    "* `master(tipo de master[x])`: If the process is being performed within a cluster, the name of the claster must be introduce as input. X indicates the numper of partitiions to divide the processing. Ideally X comes from the numer of processor cores.  Types of master:\n",
    "    * local: executes locally\n",
    "    * local[4]: executes loclaly in 4 cpus\n",
    "    * yarn: executes in hadoop batch system\n",
    "    * mesos: executes in mesos cluster\n",
    "    * spark://master:7077: execute in Spark cluster\n",
    "* `getOrCreate()`: Creates a new SparkSession based on the options provided to the builder. This function firstly checks if there is an already running Spark session and uses this one instead of creating a new one. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"Covid19_Vacunacion\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data files<a name=\"id3\"></a>\n",
    "\n",
    "Pyspark can read data in different formats like *Comma Separated Values (CSV)*, *JavaScript Object Notation (JSON)*, Parquet, etc. To read this data files you must use `spark.read`.\n",
    "\n",
    "For instance:\n",
    "\n",
    "* CSV: \n",
    "\n",
    "`csv_file = /path/to/data.csv\n",
    " data = spark.read.csv(csv_file)`\n",
    " \n",
    "* JSON:\n",
    "\n",
    "`json_file = /path/to/data.json\n",
    " data = spark.read.json(json_file)`\n",
    "\n",
    "* PARQUET:\n",
    "\n",
    "`parquet_file = /path/to/data.parquet\n",
    " data = spark.read.parquet(parquet_file)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset to use is available at [Covid 19 data repository in Github](https://github.com/owid/covid-19-data).  But it is also available in [kaggle](https://www.kaggle.com/gpreda/covid-world-vaccination-progress). \n",
    "\n",
    "El dataset contains the following attributes:\n",
    "\n",
    "Stored in [`vaccinations.csv`](vaccinations.csv) and [`vaccinations.json`](vaccinations.json). Country-by-country data on global COVID-19 vaccinations. We only rely on figures that are verifiable based on public official sources.\n",
    "\n",
    "This dataset includes some subnational locations (England, Northern Ireland, Scotland, Wales, Northern Cyprus…) and international aggregates (World, continents, European Union…). They can be identified by their `iso_code` that starts with `OWID_`.\n",
    "\n",
    "The population estimates we use to calculate per-capita metrics are all based on the last revision of the [United Nations World Population Prospects](https://population.un.org/wpp/). The exact values can be viewed [here](https://github.com/owid/covid-19-data/blob/master/scripts/input/un/population_2020.csv).\n",
    "\n",
    "* `location`: name of the country (or region within a country).\n",
    "* `iso_code`: ISO 3166-1 alpha-3 – three-letter country codes.\n",
    "* `date`: date of the observation.\n",
    "* `total_vaccinations`: total number of doses administered. For vaccines that require multiple doses, each individual dose is counted. If a person receives one dose of the vaccine, this metric goes up by 1. If they receive a second dose, it goes up by 1 again. If they receive a third/booster dose, it goes up by 1 again.\n",
    "* `total_vaccinations_per_hundred`: `total_vaccinations` per 100 people in the total population of the country.\n",
    "* `daily_vaccinations_raw`: daily change in the total number of doses administered. It is only calculated for consecutive days. This is a raw measure provided for data checks and transparency, but we strongly recommend that any analysis on daily vaccination rates be conducted using `daily_vaccinations` instead.\n",
    "* `daily_vaccinations`: new doses administered per day (7-day smoothed). For countries that don't report data on a daily basis, we assume that doses changed equally on a daily basis over any periods in which no data was reported. This produces a complete series of daily figures, which is then averaged over a rolling 7-day window. An example of how we perform this calculation can be found [here](https://github.com/owid/covid-19-data/issues/333#issuecomment-763015298).\n",
    "* `daily_vaccinations_per_million`: `daily_vaccinations` per 1,000,000 people in the total population of the country.\n",
    "* `people_vaccinated`: total number of people who received at least one vaccine dose. If a person receives the first dose of a 2-dose vaccine, this metric goes up by 1. If they receive the second dose, the metric stays the same.\n",
    "* `people_vaccinated_per_hundred`: `people_vaccinated` per 100 people in the total population of the country.\n",
    "* `people_fully_vaccinated`: total number of people who received all doses prescribed by the vaccination protocol. If a person receives the first dose of a 2-dose vaccine, this metric stays the same. If they receive the second dose, the metric goes up by 1.\n",
    "* `people_fully_vaccinated_per_hundred`: `people_fully_vaccinated` per 100 people in the total population of the country.\n",
    "* `total_boosters`: Total number of COVID-19 vaccination booster doses administered (doses\n",
    "  administered beyond the number prescribed by the vaccination protocol)\n",
    "* `total_boosters_per_hundred`: Total number of COVID-19 vaccination booster doses administered per 100 people in the total population.\n",
    "\n",
    "Note: for `people_vaccinated` and `people_fully_vaccinated` we are dependent on the necessary data being made available, so we may not be able to make these metrics available for some countries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download data\n",
    "import urllib.request\n",
    "\n",
    "url = 'https://github.com/owid/covid-19-data/raw/master/public/data/vaccinations/vaccinations.csv'\n",
    "urllib.request.urlretrieve(url, 'country_vaccinations.csv')\n",
    "\n",
    "#Load data\n",
    "\n",
    "data = spark.read.csv('country_vaccinations.csv', sep = ',', header = True, )\n",
    "type(data)\n",
    "data.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the DataFrame has been created it can be manipulated using few fuctions from *domain-specific language*, DSL from the API - Dataframe, Column, groupedData, etc. \n",
    "\n",
    "In te next subsections, the most common pyspark functions are shown, but the list of available functions is high. You can access [API Pyspark SQL](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html) to get detailed information as well as the definition of all the functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Schema<a name=\"id4\"></a>\n",
    "\n",
    "**`printSchema()`** function shows the DataFrame structure. This schema can be defined with **StrucType** that is a collection of **StructField**. Defines the name of the column (String), type of columns (DataType), if a column is *null* or not (*boolean*) and metadata if avaialable (Matadata). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some types of data that are correct to us since they appear as String. We are going to structure our data by changing the schema.\n",
    "\n",
    "You can refer to [this guide](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types) for information on the supported data types in PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "data_schema = [\n",
    "               StructField('location', StringType(), True),\n",
    "               StructField('iso_code', StringType(), True),\n",
    "               StructField('date', DateType(), True),\n",
    "               StructField('total_vaccinations', FloatType(), True),\n",
    "               StructField('people_vaccinations', FloatType(), False),\n",
    "               StructField('people_fully_vaccinated', FloatType(), True),\n",
    "               StructField('total_boosters', FloatType(), True),\n",
    "               StructField('daily_vaccinations_raw', FloatType(), True),\n",
    "               StructField('daily_vaccinations', FloatType(), True),\n",
    "               StructField('total_vaccinations_per_hundred', FloatType(), True),\n",
    "               StructField('people_vaccinated_per_hundred', FloatType(), True),\n",
    "               StructField('people_fully_vaccinated_per_hundred', FloatType(), True),\n",
    "               StructField('total_boosters_per_hundred', FloatType(), True),\n",
    "               StructField('daily_vaccinations_per_million', FloatType(), True),\n",
    "               StructField('vaccines', StringType(), True),\n",
    "               StructField('source_name', StringType(), True),\n",
    "               StructField('source_website', StringType(), True), \n",
    "            ]\n",
    "\n",
    "final_struc = StructType(fields = data_schema)\n",
    "\n",
    "data = spark.read.csv(\n",
    "    'country_vaccinations.csv',\n",
    "    sep = ',',\n",
    "    header = True,\n",
    "    schema = final_struc \n",
    "    )\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect data<a name=\"id5\"></a>\n",
    "\n",
    "Methods to read data values: schema, dtypes, show, head, first, take, describe, columns, count, distinct, printSchema. All these functions are called from an object of typr `pyspark.sql.DataFrame`.\n",
    "\n",
    "**schema**: This method is used to see the data schema (Dataframe). \n",
    " - Column name\n",
    " - Data type in column\n",
    " - nullable = true/false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**dtypes**: Returns the data type of each column as a list. String, double, int, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**head(n)**: Shows the first n rows as a *list*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**show([n])**: Show the first n lines of data in the dataFrame format.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.show()\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**first()**: Shows the first line in a row format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**take(n)**: Returns the first *n* lines of data in a list format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**describe()**: Shows an summary of statistics in data columns. \n",
    "- count: Number of values in columns\n",
    "- Mean: Mean of column data values\n",
    "- Stddev: Standard deviation of data\n",
    "- Min: Min value\n",
    "- Max: Max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**columns**: Returns the list with the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**df.column**: Select dataframe column \n",
    "\n",
    "Optionally, you can use this syntax: **df[\"columna\"]**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = data.date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**count()**: Returns the number of rows in data \n",
    "\n",
    "Can be called from a `pyspark.sql.GroupedData` object to indicate the numbre of values to group. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**distinct()**: Return the number of different rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.distinct().count()\n",
    "data.distinct().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column manipulation<a name=\"id6\"></a>\n",
    "\n",
    "In this section we will discover different methods to add, modify and delete columns in data. \n",
    "\n",
    "**Add column**: To add a new column you can call the function `withColumn(nombre_columna, data)`. This need two parameters: name of new column and data to add. \n",
    "\n",
    "In this case, we can create a new column duplicating the information of date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn(\"new_date\", data[\"date\"])\n",
    "\n",
    "data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Add a new column with the different bewtween vaccinations from previous and current date. We will use the following methods:\n",
    "\n",
    "1. pyspark.sql.window \n",
    "2. pyspark.sql.functions.lag\n",
    " pyspark.sql.functions.lag(col, offset=1, default=None)[source]\n",
    "\n",
    "    Window function: returns the value that is offset rows before the current row, and defaultValue if there is less than offset rows before the current row. For example, an offset of one will return the previous row at any given point in the window partition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag, col\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "w = Window().partitionBy().orderBy(col(\"date\"))\n",
    "\n",
    "data = data.withColumn('prev_daily_vac',\n",
    "                        lag(data['daily_vaccinations'])\n",
    "                                 .over(w))\n",
    "\n",
    "data = data.withColumn(\"vaccination_difference\", data[\"daily_vaccinations\"] - data[\"prev_daily_vac\"])\n",
    "\n",
    "data.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modify column**: To rename a column, we use the method `withColumnRenamed(old_name, new_name)`. This function just take as input parameters the name of the column to rename and the new name for the column. \n",
    "\n",
    "Vamos a renombrar múltiples columnas para hacerlas más entendibles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1ª opción utilizando el método withColumnRenamed:\n",
    "\n",
    "data = data.withColumnRenamed(\"source_website\", \"source_url\") \\\n",
    "\n",
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove column**: With `drop(columnName)` the column `columnName` is deleted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos aquellas columnas que no nos interesan\n",
    "data = data.drop(\"iso_code\")\n",
    "\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust null or empty values\n",
    "\n",
    "When a dataset from external sources is used, it is common to find empty or null values. One of the most common techniques to manipulate data is to remove these kind of values. This values may be null, NaN, empty, etc.\n",
    "\n",
    "To check is a column has a null value, pySpark include the following functions: \n",
    "* **isnan(column)** is the function from the pyspark.sql.functions package to know if a column included nan values.\n",
    "* **isNull()** is the function from the pyspark.sql.column to know if a column included null values.\n",
    "\n",
    "There are different techniques to avoid this problem.\n",
    "\n",
    "- **Delete** those rows with nulls in any column df.na.drop o data.nadrop()\n",
    "- **Replace** an empty column in a row with 0.0 or big value or **Replace** data with **mean** o **meadian**.\n",
    "- Select **most frequent** values from a column. It works fine with well classified data but it may introduce some *bias*. \n",
    "- Using **KNN**. *K-Nearest Neighbors* algorithm that uses classification to new data usinf some distance mertrics like Euclídea, Mahalanobis, Manhattan, Minkowski, Hamming, etc. This is the most effective method, but also the mos computionally difficult, and it must be understood before appllying. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "# Replace NA by Mean\n",
    "#data.na.fill(data.select(mean(data['total_vac'])).collect()[0][0])\n",
    "\n",
    "# Reemplazar los valores null por un nuevo valor\n",
    "data = data.na.fill(0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data queries\n",
    "\n",
    "PySpark API and PySpark SQL includes a set of metos to perform queries over data, in a more or less easy way: *select, filter, between, when, like, groupby, aggregations*.  \n",
    "\n",
    "**select(nombreColumna)**: It is used to show one or few columns receiving the name of the columns as a parameter. `pyspark.sql.DataFrame.select`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select(\"total_vaccinations\").show(2)\n",
    "data.select(\"location\", \"date\", \"total_vaccinations\", \"daily_vaccinations\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**filter(condition)**: Filter data based on a given condition. Different conditions can be set ising the following operands: AND(&), OR(|), o NOT(~). `pyspark.sql.DataFrame.filter`\n",
    "\n",
    "What is selected in the following query?\n",
    "\n",
    "\\* `lit()` creates a new literal vale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "data.filter( (col('date') >= lit('2021-02-01')) & (col('date') <= lit('2021-02-15')) ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**between(low_value,high_value)**: Returns *true* or *false* based on the values provided as parameters. `pyspark.sql.Column.between`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Las filas con los valores de la columna vacunaciones_dia devolverán True como condición de filter.\n",
    "data.filter( data.daily_vaccinations.between(100.0, 1000.0) ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**when (condicion, valor)**: Function that returns `value` or `null` depending if the condition is true. Function of `pyspark.sql.functions.when`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra los valores \n",
    "from pyspark.sql.functions import when\n",
    "data.select(\"location\",\"total_vaccinations\", \"total_boosters_per_hundred\", when(data.daily_vaccinations >= 1000.0, 1).otherwise(0).alias(\"daily_vaccinations\")).show(10)\n",
    "data.select(\"location\",\"total_vaccinations\", \"total_boosters_per_hundred\", when(data.daily_vaccinations >= 1000.0, 1).alias(\"daily_vaccinations\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**like(expression)**: Returns values of column under a certain expression. `pyspark.sql.Column.like`\n",
    "\n",
    "\\* Use `rlike()` or `like()` for a regular expression.\n",
    "Location starting by S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.filter(col(\"location\").like('S%')).select('location').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**groupBy(column_name)**: Group data by *column_name* introduced as input. This returns an object of `pyspark.sql.GroupedData` type that can be analized with mean, min, max, count, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean by location\")\n",
    "data.groupBy('location').mean().show(5, truncate=False)\n",
    "print(\"MIN by location\")\n",
    "data.select(\"location\",\"total_vaccinations\", \"people_vaccinations\", \"people_fully_vaccinated\", \"daily_vaccinations\", \"total_vaccinations_per_hundred\").groupBy('location').min().show(5)\n",
    "print(\"MAX by location\")\n",
    "data.select(\"location\",\"total_vaccinations\", \"people_vaccinations\", \"people_fully_vaccinated\", \"daily_vaccinations\", \"total_vaccinations_per_hundred\").groupBy('location').max().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Aggregation**: Aggregation functions `agg` are used to join two operations in a column. This functions operates over a group of rows (groupedData) and calculates a unique value over that group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, mean, col,lit\n",
    "\n",
    "data.filter( (col('date') >= lit('2021-02-01')) & (col('date') <= lit('2021-02-24')) )\\\n",
    "    .orderBy(\"location\")\\\n",
    "    .groupBy(\"location\")\\\n",
    "    .agg(min(\"date\").alias(\"Date from\"),\n",
    "         max(\"date\").alias(\"date to\"),\n",
    "         min(\"total_vaccinations\").alias(\"Total Vaccinated from\"),\n",
    "         max(\"total_vaccinations\").alias(\"Total vaccinates to\"),\n",
    "         mean(\"daily_vaccinations\").alias(\"Mean of daily vaccinations\")\n",
    "        )\\\n",
    "    .show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data handling exercises: \n",
    "\n",
    "1. TOP 20 of most vaccinated. \n",
    "    1. Number of vaccinations\n",
    "    2. Ratio of vaccinations \n",
    "    3. Data visualization\n",
    "3. Vaccination data from any location in the last 4 weeks. \n",
    "    3. Visualize the vaccination progress regarding total of vaccinations\n",
    "    4. **How many** locations are better?\n",
    "4. Which is the country which has the best daily vaccination rate?\n",
    "    1. Which is the country with the best vaccination rate today (or yesterday)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. TOP 20 de los países con mayor número de vacunas suministradas.\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql.functions import max, mean, desc\n",
    "#data.select('pais', 'total_vac', 'nombre_vacunas')\\\n",
    "#                   .groupBy('pais')\\\n",
    "#                   .agg(max('total_vac').alias(\"max_vac\"))\\\n",
    "#                   .orderBy(desc('max_vac')).show(20,truncate=False)\n",
    "\n",
    "sec_df =  data.select('location', 'total_vaccinations')\\\n",
    "                   .groupBy('location')\\\n",
    "                   .agg(max('total_vaccinations').alias(\"max_vac\"))\\\n",
    "                   .orderBy(desc('max_vac'))\\\n",
    "                   .limit(20)\\\n",
    "                   .toPandas()\n",
    "\n",
    "sec_df.plot(kind = 'bar', x='location', y = sec_df.columns.tolist()[1:], \n",
    "                    figsize=(12, 6), ylabel = 'Total Vaccinations', xlabel = 'Locations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    1. ratio del total de vacunaciones\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql.functions import max, mean, desc\n",
    "data.select('location', 'total_vaccinations', \"people_vaccinated_per_hundred\")\\\n",
    "                   .groupBy('location')\\\n",
    "                   .agg(max('people_vaccinated_per_hundred').alias(\"max_ratio_vac\"))\\\n",
    "                   .orderBy(desc('max_ratio_vac')).show(2,truncate=False)\n",
    "\n",
    "sec_df =  data.select('location', 'total_vaccinations', \"people_vaccinated_per_hundred\")\\\n",
    "                   .groupBy('location')\\\n",
    "                   .agg(max('people_vaccinated_per_hundred').alias(\"max_ratio_vac\"))\\\n",
    "                   .orderBy(desc('max_ratio_vac'))\\\n",
    "                   .limit(20)\\\n",
    "                   .toPandas()\n",
    "\n",
    "sec_df.plot(kind = 'bar', x='location', y = sec_df.columns.tolist()[1:], \n",
    "                    figsize=(12, 6), ylabel = 'Ratio Total Vacunaciones', xlabel = 'Paises')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. ratio del total de personas vacunadas \n",
    "\n",
    "from pyspark.sql.functions import max, mean, desc\n",
    "data.select('location', 'total_vaccinations', \"people_fully_vaccinated_per_hundred\")\\\n",
    "                   .groupBy('location')\\\n",
    "                   .agg(max('people_fully_vaccinated_per_hundred').alias(\"max_ratio_personas\"))\\\n",
    "                   .orderBy(desc('max_ratio_personas')).show(2,truncate=False)\n",
    "\n",
    "sec_df =  data.select('location', 'total_vaccinations', \"people_fully_vaccinated_per_hundred\")\\\n",
    "                   .groupBy('location')\\\n",
    "                   .agg(max('people_fully_vaccinated_per_hundred').alias(\"max_ratio_personas\"))\\\n",
    "                   .orderBy(desc('max_ratio_personas'))\\\n",
    "                   .limit(20)\\\n",
    "                   .toPandas()\n",
    "\n",
    "sec_df.plot(kind = 'bar', x='location', y = sec_df.columns.tolist()[1:], \n",
    "                    figsize=(12, 6), ylabel = 'Ratio Total Vacunaciones', xlabel = 'Paises')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spain in the last weeks\n",
    "data.select('location', 'date', 'total_vaccinations', 'people_vaccinations', 'people_fully_vaccinated', 'daily_vaccinations', 'total_vaccinations_per_hundred', 'people_vaccinated_per_hundred', 'people_fully_vaccinated_per_hundred', 'daily_vaccinations_per_million')\\\n",
    "    .filter((data.location==\"Spain\") & ((data.date >= lit('2021-02-10')) & ((data.date) <= lit('2021-02-24'))))\\\n",
    "    .show()\n",
    "\n",
    "spain_data = data.select('location', 'date', 'total_vaccinations', 'people_vaccinations', 'people_fully_vaccinated', 'daily_vaccinations', 'total_vaccinations_per_hundred', 'people_vaccinated_per_hundred', 'people_fully_vaccinated_per_hundred', 'daily_vaccinations_per_million')\\\n",
    "                 .filter((data.location==\"Spain\") & ((data.date >= lit('2021-02-10')) & ((data.date) <= lit('2021-02-24'))))\n",
    "\n",
    "\n",
    "    #Visualizar el progreso de vacunación en España en función del total de vacunas suministradas desde el principio de vacunación. \n",
    "    # Corregir los valores nulos para que el gráfico no se desvirtue\n",
    "import matplotlib.pyplot as plt \n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "spain_data = spain_data.withColumn(\"total_vaccinations\", when(((spain_data.date == lit('2021-02-12')) | (spain_data.date == lit('2021-02-13'))),2423045.0).otherwise(spain_data.total_vaccinations))\n",
    "spain_data = spain_data.withColumn(\"total_vaccinations\", when(((spain_data.date == lit('2021-02-19')) | (spain_data.date == lit('2021-02-20'))),2936011.0).otherwise(spain_data.total_vaccinations))\n",
    "    \n",
    "spain_data.show()\n",
    "\n",
    "spain_data_pandas = spain_data.toPandas()\n",
    "\n",
    "plt.plot(spain_data_pandas.date, spain_data_pandas.total_vaccinations)\n",
    "\n",
    "plt.xlabel(\"Fecha\")\n",
    "plt.ylabel(\"Total Vacunaciones (España)\")\n",
    "plt.title(\"Progresión de las vacunaciones en España\")\n",
    "plt.xticks(rotation=30)\n",
    "    \n",
    "    #¿Cuántos países están por delante de España en la vacunación?\n",
    "from datetime import date\n",
    "today = date.today()\n",
    "\n",
    "# dd/mm/YY\n",
    "#today = today.strftime(\"%Y-%m-%d\")\n",
    "#print(\"d1 =\", today)\n",
    "\n",
    "spain_vac = spain_data.filter(spain_data.date == lit('2021-02-21')).select(\"total_vaccinations\").head()\n",
    "total_vac = data.select('location', 'total_vaccinations')\\\n",
    "                  .groupBy('location')\\\n",
    "                  .agg(max('total_vaccinations').alias(\"max_vac\"))\\\n",
    "                  .orderBy(desc('max_vac'))\n",
    "\n",
    "print(\"Las vacunaciones en España son %d\"% spain_vac)\n",
    "total_vac.select('location', 'max_vac')\\\n",
    "    .filter(total_vac.max_vac >= spain_vac[0])\\\n",
    "    .orderBy(desc('max_vac')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "#4. ¿Cuál es el país que vacuna más gente al día?\n",
    "data.select('location', 'daily_vaccinations')\\\n",
    "    .groupBy('location')\\\n",
    "    .agg(mean('daily_vaccinations').alias(\"mean_vac\"))\\\n",
    "    .orderBy(desc('mean_vac')).show(20,truncate=False)\n",
    "\n",
    "#    ¿Cuál es el país que vacunó más gente el última día (del que hay records)?\n",
    "data.filter(data.date==\"2021-02-22\").groupBy(\"location\", \"total_vaccinations\")\\\n",
    "    .agg(max(\"total_vaccinations\").alias(\"max_vac\")).orderBy(desc(\"max_vac\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Library (MLlib)\n",
    "\n",
    "We will apply the K-means Algorithm for our analysis, so we must import that library.\n",
    "\n",
    "The data must be transform to one single column when every row in our DataFrame contains a vector using the function `VectorAssembler`. To create clusters in K-means we need to select the columns based on the parameters that we need to predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In K means, we can select N dimensions to find clusters. This time, we will try to define the space taking into account the % of vaccinations per country and the daily vaccinations per million. This way, we will find groups of countries.\n",
    "\n",
    "Let's prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "df = data.select('location', 'people_fully_vaccinated_per_hundred', 'daily_vaccinations_per_million').groupBy(\"location\")\\\n",
    "    .agg(max(\"people_fully_vaccinated_per_hundred\").alias(\"max_vac\"), mean(\"daily_vaccinations_per_million\").alias(\"mean_daily_vac\")).orderBy(desc(\"max_vac\"))\n",
    "# Tener en cuenta que los datos no sean strings\n",
    "feat_cols = [\"max_vac\",  \"mean_daily_vac\"]\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol='features')\n",
    "\n",
    "df = df.withColumnRenamed(\"location\", \"id\")\n",
    "df = vec_assembler.transform(df)\n",
    "df.show()\n",
    "\n",
    "for_plot = df.toPandas()\n",
    "\n",
    "plt.plot(for_plot.max_vac,  for_plot.mean_daily_vac, 'o', color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run k-means we just need to have two columns, id (or location) and features. Let's drop the rest and rename location to id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_data = df.drop('max_vac', 'mean_daily_vac')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize choice of k\n",
    "One disadvantage of KMeans compared to more advanced clustering algorithms is that the algorithm must be told how many clusters, k, it should try to find. To optimize k we cluster a fraction of the data for different choices of k and look for an \"elbow\" in the cost function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "\n",
    "cost = np.zeros(20)\n",
    "for k in range(2,20):\n",
    "    kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "    model = kmeans.fit(km_data.sample(False,0.1, seed=42))\n",
    "    # Make predictions\n",
    "    predictions = model.transform(km_data)\n",
    "\n",
    "    # Evaluate clustering by computing Silhouette score\n",
    "    evaluator = ClusteringEvaluator()\n",
    "\n",
    "    silhouette = evaluator.evaluate(predictions)\n",
    "    print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "kmeans = KMeans().setK(k).setSeed(1).setFeaturesCol(\"features\")\n",
    "model = kmeans.fit(km_data)\n",
    "centers = model.clusterCenters()\n",
    "\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign clusters to events\n",
    "Their is one import thing left to do; assigning the individual rows to the nearest cluster centroid. That can be done with the transform method, which adds 'prediction' column to the dataframe. The prediction value is an integer between 0 and k, but it has no correlation to the y value of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = model.transform(km_data).select('id', 'prediction')\n",
    "rows = transformed.collect()\n",
    "print(rows[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the rows returned by the collect method it is trivial to create a new dataframe using our SQL context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(spark)\n",
    "df_pred = sqlContext.createDataFrame(rows)\n",
    "df_pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df_pred.join(df, 'id')\n",
    "df_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Pandas dataframe\n",
    "Typically at this point I would need to do something else with the data, which does not require Spark, so let's convert the Spark dataframe to a good old Pandas dataframe for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pddf_pred = df_pred.toPandas().set_index('id')\n",
    "pddf_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the results\n",
    "The final step is to visually inspect the output to see if the KMeans model did a good job or not. Comparing with the first figure it is clear that most clusters were indeed found, but the left blue cluster should have been split in two and the orange+brown clusters should have been only one cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threedee = plt.figure(figsize=(12,10)).gca(projection='3d')\n",
    "threedee.scatter(pddf_pred.max_vac, pddf_pred.mean_daily_vac, c=pddf_pred.prediction)\n",
    "threedee.set_xlabel('max_vac')\n",
    "threedee.set_ylabel('mean_daily_vac')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = pddf_pred.prediction\n",
    "x = pddf_pred.max_vac\n",
    "y = pddf_pred.mean_daily_vac\n",
    "\n",
    "plt.scatter(x, y, c=colors)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
